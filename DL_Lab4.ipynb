{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMzWMHJRPJN1OEbbwXrrgxX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TigistW/Deep-Learning-Labs/blob/main/DL_Lab4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 333,
      "metadata": {
        "id": "7-GhotXO1sn3"
      },
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lyYNrSlTCr-W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DenseLayer:\n",
        "    def __init__(self, n_features, n_neurons):\n",
        "        self.weights = 0.01 * torch.rand(n_features, n_neurons)\n",
        "        self.biases = torch.zeros(1, n_neurons)\n",
        "        self.num_neurons = n_neurons\n",
        "        self.output = None\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        self.output = torch.matmul(inputs, self.weights) + self.biases\n",
        "\n",
        "\n",
        "class Activation_ReLU:\n",
        "    def forward(self, inputs):\n",
        "        self.output = torch.max(torch.tensor(0.0), inputs)\n",
        "class Activation_Linear:\n",
        "    def forward(self, inputs):\n",
        "        self.output = inputs\n",
        "\n",
        "class Activation_Sigmoid:\n",
        "    def forward(self, inputs):\n",
        "        self.output = 1 / (1 + torch.exp(-inputs))\n",
        "\n",
        "class Activation_Softmax:\n",
        "    def forward(self, inputs):\n",
        "        exp_values = torch.exp(inputs) - torch.max(inputs, axis=1, keepdims=True).values\n",
        "        summed_exp = torch.sum(exp_values, axis=1, keepdims=True)\n",
        "        self.output = exp_values / summed_exp\n",
        "\n",
        "class Loss_CategoricalCrossentropy:\n",
        "    def __init__(self, y_pred, y_true):\n",
        "      self.loss = None\n",
        "      # added clip to avoid undefined values\n",
        "      y_pred = torch.clip(y_pred, 1e-8, 1 - 1e-8)\n",
        "\n",
        "      if len(y_true.shape) == 2:\n",
        "        self.forward_with_hot_encoding(y_pred, y_true)\n",
        "      elif len(y_true.shape) == 1:\n",
        "        self.forward_without_encoding(y_pred, y_true)\n",
        "      else:\n",
        "        pass\n",
        "\n",
        "    def forward_without_encoding(self, y_pred, y_true):\n",
        "        correct_confidences = y_pred[range(len(y_pred)), y_true]\n",
        "        log_loss = -torch.log(correct_confidences)\n",
        "        self.loss = torch.mean(log_loss)\n",
        "\n",
        "    def forward_with_hot_encoding(self, y_pred, y_true):\n",
        "        correct_confidences = torch.sum(y_pred * y_true, axis=1)\n",
        "        log_loss = -torch.log(correct_confidences)\n",
        "        self.loss = torch.mean(log_loss)\n",
        "\n",
        "class NeuralNetwork:\n",
        "    def __init__(self, input, num_features, num_neurons, activation_function):\n",
        "\n",
        "        self.num_features = num_features\n",
        "        self.activation = activation_function\n",
        "        self.num_neurons = num_neurons\n",
        "        self.input = input\n",
        "        self.output = None\n",
        "        self.layer = None\n",
        "\n",
        "    def forward(self):\n",
        "        self.layer = DenseLayer(self.num_features, self.num_neurons)\n",
        "        self.layer.forward(self.input)\n",
        "        self.activation.forward(self.layer.output)\n",
        "        self.output = self.activation.output\n",
        "\n",
        "class Accuracy:\n",
        "  # included one hot encoded handling mech in this implementation\n",
        "  def acc_percentage(self, predicted, target):\n",
        "      total_samples = len(target)\n",
        "      predictions = torch.argmax(predicted, axis=1)\n",
        "      print(\"predictions\", predictions[:10])\n",
        "      if len(target.shape) == 2:\n",
        "        target = torch.argmax(target, axis=1)\n",
        "      print(target[:10])\n",
        "      self.accuracy = torch.mean((predictions == target).float()) * 100.0\n",
        "      print(self.accuracy)\n"
      ],
      "metadata": {
        "id": "U12eRAA44LM-"
      },
      "execution_count": 334,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def forward_pass(input):\n",
        "  hidden_layer_1.forward(input)\n",
        "  activation1.forward(hidden_layer_1.output)\n",
        "  output_layer.forward(activation1.output)\n",
        "  activation_2.forward(output_layer.output)\n",
        "  return activation_2.output\n"
      ],
      "metadata": {
        "id": "-4bA-oKaRxj_"
      },
      "execution_count": 335,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def linear_derivative(x):\n",
        "    return torch.ones_like(x)\n",
        "def sigmoid_derivative(x):\n",
        "    return x * (1 - x)\n",
        "\n",
        "def back_prop(fp):\n",
        "\n",
        "      lr = torch.tensor(0.01)\n",
        "\n",
        "      # output layer gradients with linear activation\n",
        "      output_delta_1 = (fp[0][0] - y[0]) * linear_derivative(fp[0][0])\n",
        "      output_delta_2 = (fp[0][1] - y[1]) * linear_derivative(fp[0][1])\n",
        "\n",
        "      # update output layer weights and biases\n",
        "      for i in range(4):\n",
        "          output_layer.weights[i][0] -= lr * output_delta_1 * activation1.output[0][i]\n",
        "          output_layer.weights[i][1] -= lr * output_delta_2 * activation1.output[0][i]\n",
        "\n",
        "      output_layer.biases[0][0] -= lr * output_delta_1\n",
        "      output_layer.biases[0][1] -= lr * output_delta_2\n",
        "\n",
        "      # hidden layer gradients\n",
        "      hidden_delta_1 = sigmoid_derivative(hidden_layer_1.output[0][0]) * (\n",
        "          output_delta_1 * output_layer.weights[0][0] + output_delta_2 * output_layer.weights[1][0]\n",
        "      )\n",
        "      hidden_delta_2 = sigmoid_derivative(hidden_layer_1.output[0][1]) * (\n",
        "          output_delta_1 * output_layer.weights[0][1] + output_delta_2 * output_layer.weights[1][1]\n",
        "      )\n",
        "      hidden_delta_3 = sigmoid_derivative(hidden_layer_1.output[0][2]) * (\n",
        "          output_delta_1 * output_layer.weights[2][0] + output_delta_2 * output_layer.weights[3][0]\n",
        "      )\n",
        "      hidden_delta_4 = sigmoid_derivative(hidden_layer_1.output[0][3]) * (\n",
        "          output_delta_1 * output_layer.weights[2][1] + output_delta_2 * output_layer.weights[3][1]\n",
        "      )\n",
        "\n",
        "      # update hidden layer weights and biases\n",
        "      for i in range(2):\n",
        "          hidden_layer_1.weights[i][0] -= lr * hidden_delta_1 * X[i]\n",
        "          hidden_layer_1.weights[i][1] -= lr * hidden_delta_2 * X[i]\n",
        "          hidden_layer_1.weights[i][2] -= lr * hidden_delta_3 * X[i]\n",
        "          hidden_layer_1.weights[i][3] -= lr * hidden_delta_4 * X[i]\n",
        "\n",
        "      hidden_layer_1.biases[0][0] -= lr * hidden_delta_1\n",
        "      hidden_layer_1.biases[0][1] -= lr * hidden_delta_2\n",
        "      hidden_layer_1.biases[0][2] -= lr * hidden_delta_3\n",
        "      hidden_layer_1.biases[0][3] -= lr * hidden_delta_4\n"
      ],
      "metadata": {
        "id": "esqHXFxdyIj6"
      },
      "execution_count": 336,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def error_calculation(y_true, y_pred):\n",
        "  return torch.mean(0.5*(y_true - y_pred)**2)"
      ],
      "metadata": {
        "id": "J0YnLVE7zA-a"
      },
      "execution_count": 337,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = torch.tensor([0.1, 0.1])\n",
        "y = torch.tensor([0.5, 0.8])\n",
        "\n",
        "hidden_layer_1 = DenseLayer(2, 4)\n",
        "activation1 = Activation_Sigmoid()\n",
        "output_layer = DenseLayer(4, 2)\n",
        "activation_2 = Activation_Linear()"
      ],
      "metadata": {
        "id": "MabzA_ju7m1d"
      },
      "execution_count": 338,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss = 0.00001"
      ],
      "metadata": {
        "id": "kDKiNRczzDaJ"
      },
      "execution_count": 339,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# calculate prediction and error of the prediction\n",
        "y_pred = forward_pass(X)\n",
        "err = error_calculation(y, y_pred)\n",
        "\n",
        "print(\"Initial loss:\", err, \"Initial prediction\", y_pred)\n",
        "\n",
        "while err > loss:\n",
        "  back_prop(y_pred)\n",
        "  y_pred = forward_pass(X)\n",
        "  err = error_calculation(y, y_pred)\n",
        "print(\"Final loss:\", err)\n",
        "print(\"Final prediction:\",y_pred)\n",
        "print(\"Correct value:\",y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1IDGlf12zDtY",
        "outputId": "058abc25-eb07-4e40-a599-3bbeb836d12f"
      },
      "execution_count": 340,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial loss: tensor(0.2164) Initial prediction tensor([[0.0100, 0.0092]])\n",
            "Final loss: tensor(9.9913e-06)\n",
            "Final prediction: tensor([[0.4967, 0.7946]])\n",
            "Correct value: tensor([0.5000, 0.8000])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7ocDhgC39jH3"
      },
      "execution_count": 340,
      "outputs": []
    }
  ]
}